 The last time I stood on this stage in the town hall theatre was 26 years ago. I was a handsome 12-year-old boy. I was in a drama competition for national schools in a play written by my best friend. In that play I was a detective trying to solve a mystery of who robbed a fictional hotel. The hotel was called Hotel El Chico Nanzipo. I was also a boy with a stammer or a speech impediment and I was desperately trying to both remember and say my lines. During that play I spent 70% of my time collecting information to solve this mystery of who robbed the hotel. So fast forward 26 years and not much has changed. I'm now working as a medical consultant in the hospital for half of my time and as a senior lecture in applied clinical data analytics in the university for the other half. The context has changed from a detective solving a mystery of who robbed a hotel to a doctor solving a mystery what is the cause of illness in this patient in front of me. And I still spend 70% of my time collecting information about the patient and only 30% of my time making decisions on that information and communicating with the patient. So the information or data that I collect takes many forms, patients blood pressure, medical history, blood test results. And this imbalance in how health care is delivered this 70-30 is well known all over the world. And in many specialities it's been made worse by technology with the introduction of the electronic health record for example that was designed for collecting information about billing and not designed to make health care more efficient. So this extra administrative workload that doctors have to do reduces the face time that they have with patients. The face time that we were fundamentally trained to do and the face time that the patients want. So the idea we're spreading that I'm going to share tonight is a potential solution to this. We've all heard a lot about the risks of artificial intelligence but I want to introduce a new perspective. One where the responsible use of medical AI could help to solve some of these problems. And I'm going to introduce a new type of AI called multimodal AI. Multi-modal AI is AI that takes data in many different forms, text, images, numbers. When I work as a doctor in the hospital I'm talking to the patient, I'm listening to the patient, I'm listening to their chest with a status scope, I'm palpating their abdomen, I'm looking at their blood test results. This is multimodal human intelligence. Multi-modal meaning lots of different types of data. So in November last year the media coverage of AI really exploded with the release of ChatGPT by OpenAI. So ChatGPT is a type of AI called the large language model or a generative AI. But it's not the only type of AI. There's other types of AI that are less familiar to the general public like machine learning, computer vision, natural language processing. And those AI is mostly taken a single type of data and we call this single model AI. So I'm going to give three cutting edge examples of single model AI in healthcare. The image behind me is an image of a chest X-ray with a heart in the center surrounded by the lungs, the ribs going across the shoulders at the top. AI has become really good at distinguishing normal from abnormal. We call this triaging. And most X-rays done in the world are actually normal. So this software called ChestLink by a company called OxyPis. It's a medical AI triage system. And it's the first system that's received regulatory or CE approval to be used in a fully autonomous way to report on chest X-rays. So what OxyPis does is it looks for 75 abnormalities on the chest X-ray. And if it doesn't find any of those abnormalities, it reports the X-ray as normal without any human involvement. If it does find an abnormality, it passes the X-ray back to the human radiologist to report. This is an example of task sharing between the AI and the human radiologist. The picture behind me is a retina. This is the tissue at the back of your eye. If you've ever been for an eye test, this is what the optician sees. The optician is looking for reversible causes of blindness like macular degeneration. A group of researchers in University College London developed an AI model trained on 1.6 million pictures of the retina. This model is able to diagnose eye disease and predict outcomes from eye conditions like macular degeneration. And that's very impressive that it can do what most non-specialist doctors find hard, but it doesn't stop there. If you think about a condition like Parkinson's disease, we don't think about the back of the eye. Parkinson's disease affects your movement, causes a tremor, affects your walking. The same AI model can have a look at the back of your eye and predict Parkinson's disease years before patients develop symptoms. So now not only is it kind of see what the human can see, but it can see things that the human can see. However, this model will never diagnose Parkinson's disease and it definitely will never give compassionate care for Parkinson's disease. AI like this must be used in conjunction with highly trained healthcare professionals. I can move away from computer vision towards large language models. In December of last year, Google released a medical large language model called MedPam. They trained their generic large language model called PAM to perform medical question and answering. And this is the first time ever that a computer or an AI model has passed a US medical licensing exam with a passing score of 67%. And then only three months later, MedPam 2, the next version, got a score of 86%. This is expert level on that exam. If you have a smartphone in your pocket, multimodal AI is available to you right now. Four weeks ago, OpenAI released a multimodal version of chat GPT. And this is an example I gave it last week, where I passed in a picture of an ECG. This is the electrical activity of the heart, a very common test that we do in the hospital. And I gave it a little scenario. Six-year-old male presented with palpitations. That's a sensation of your heart beating in your chest. He could feel his heart skipping beats. No past medical history, not currently on medications. The attached picture is ECG. What is the next step for this patient? Now, very unhelpful for this presentation. Chat GPT told me that it's a machine learning model and not a physician. And it can't give me medical advice. So, I asked it to help a friend out. And told us I'm doing a TEDx talk about multimodal AI. And please play along. And it did exactly that. Now, although the ECG analysis wasn't perfect, it was very, very close. And the follow-up advice that it gave was perfect. But this gets even better when multimodal large language models are trained on medical tasks. And the best example of this is MedPamM, M for multimodal, released by Google in July of this year. MedPamM takes multiple different types of input. Pictures of the skin, pictures of chest x-rays, pictures of patology, texts from radiology images. And performs multiple medical tasks. So, it's not perfect, but the radiology report that generated from MedPamM was compared to a human radiologist report. And the blinded assessors preferred the MedPamM report in 40% of cases. So, the things we need to implement multimodal AI safely are trust, explainability, and randomized clinical trials. In relation to trust, there was a survey done in the United States. And over half of their respondents would feel anxious if they knew their healthcare worker was relying on AI for their medical treatment. In the same survey, 75% of the respondents feared that doctors were going to integrate AI too quickly before understanding the risks to patients. So, we have a lot of work to do to bridge this gap. The second thing is explainability. Explainable AI opens up the black box to tell us why it met the output. So, in our research, what we're interested in is why did the AI model pick a particular blood pressure medication for high blood pressure? Should we just go along with what the model says, or do we want to know why it got to that conclusion? If the output from the model agrees with our world view, then we might just go along with it and not question it. And that's a very risky area in medicine called confirmation bias. The third thing we need is randomized clinical trials. AI models must be tested in the same way that we test medicines. For medicines, we use randomized controlled trials. This is the peak of evidence in medicine. One group receiving the AI model, another group not receiving the AI model, and follow them up to see who does better. So, what's the missing piece? Where does the art of medicine fit in? As medical students and junior doctors, we're always taught to first look at the patient. You never interpret a result. An x-ray, an ECG, without knowing the context of the patient. We often call this the eyeball test. And this has been tested where patients coming to emergency departments, nurses would try to age them as red, yellow, or green from just looking at the patient. And this was shown to be more accurate than sophisticated models. So, in the future, I see a world where a picture or a video of the patient is also fed into the multimodal model. So, now, looking back at myself, my 12-year-old self, standing nervously on that stage, I can see the parallels between then and now. Back then, I was looking for data to solve the mystery of who robbed the fictional hotel, but now we're looking for data for a more profound reason. And that's to make healthcare more efficient, personalized, and accessible. Imagine a world where remote corners of low and middle-income countries that have no access to specialized care can gain insights from these models. That's a world that medical AI and especially multimodal medical AI can help us create. So, as we look to the future, we have to prioritize compassion and understanding. We have to build this relationship between AI and the humans to allow the doctors more time to spend with the patients to understand them and give them a better chance at health and happiness. Thank you.